{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ykKatoB3UuYC"
      },
      "outputs": [],
      "source": [
        "# Solving for residual std scaling issue\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9WN41HifUw3R"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6TqA3giYUw6K"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdk00btkUw9Z"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwYHATN9UxAp",
        "outputId": "949fcb85-6431-4cf8-a3ef-9b080984b3e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# SEED\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# STOP\n",
        "num_return_sequences = 5\n",
        "max_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5elZnJNoU9Tb"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # at init load tokens from disk and store them in memory\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "        # state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B*T\n",
        "        # if loading the next batch would be out of bounds, reset\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4E0FHR67U9Yy",
        "outputId": "9209c509-7a3d-485e-89ce-1a090018f061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "step    10 | loss: 2.635145 | lr: 1.20e-05\n",
            "step    20 | loss: 2.401118 | lr: 2.40e-05\n",
            "step    30 | loss: 2.274045 | lr: 3.60e-05\n",
            "step    40 | loss: 2.218190 | lr: 4.80e-05\n",
            "step    50 | loss: 2.175796 | lr: 6.00e-05\n",
            "step    60 | loss: 2.156320 | lr: 7.20e-05\n",
            "step    70 | loss: 2.091089 | lr: 8.40e-05\n",
            "step    80 | loss: 2.044581 | lr: 9.60e-05\n",
            "step    90 | loss: 1.987873 | lr: 1.08e-04\n",
            "step   100 | loss: 1.944466 | lr: 1.20e-04\n",
            "step   110 | loss: 1.859326 | lr: 1.32e-04\n",
            "step   120 | loss: 1.805976 | lr: 1.44e-04\n",
            "step   130 | loss: 1.725748 | lr: 1.56e-04\n",
            "step   140 | loss: 1.678598 | lr: 1.68e-04\n",
            "step   150 | loss: 1.607366 | lr: 1.80e-04\n",
            "step   160 | loss: 1.572116 | lr: 1.92e-04\n",
            "step   170 | loss: 1.530479 | lr: 2.04e-04\n",
            "step   180 | loss: 1.518564 | lr: 2.16e-04\n",
            "step   190 | loss: 1.481580 | lr: 2.28e-04\n",
            "step   200 | loss: 1.477091 | lr: 2.40e-04\n",
            "step   210 | loss: 1.446707 | lr: 2.52e-04\n",
            "step   220 | loss: 1.466999 | lr: 2.64e-04\n",
            "step   230 | loss: 1.430226 | lr: 2.76e-04\n",
            "step   240 | loss: 1.425198 | lr: 2.88e-04\n",
            "step   250 | loss: 1.394518 | lr: 3.00e-04\n",
            "step   260 | loss: 1.395756 | lr: 3.12e-04\n",
            "step   270 | loss: 1.378392 | lr: 3.24e-04\n",
            "step   280 | loss: 1.367542 | lr: 3.36e-04\n",
            "step   290 | loss: 1.326080 | lr: 3.48e-04\n",
            "step   300 | loss: 1.323688 | lr: 3.60e-04\n",
            "step   310 | loss: 1.326609 | lr: 3.72e-04\n",
            "step   320 | loss: 1.309921 | lr: 3.84e-04\n",
            "step   330 | loss: 1.278332 | lr: 3.96e-04\n",
            "step   340 | loss: 1.250937 | lr: 4.08e-04\n",
            "step   350 | loss: 1.271186 | lr: 4.20e-04\n",
            "step   360 | loss: 1.270979 | lr: 4.32e-04\n",
            "step   370 | loss: 1.218472 | lr: 4.44e-04\n",
            "step   380 | loss: 1.203370 | lr: 4.56e-04\n",
            "step   390 | loss: 1.231225 | lr: 4.68e-04\n",
            "step   400 | loss: 1.231256 | lr: 4.80e-04\n",
            "step   410 | loss: 1.175044 | lr: 4.92e-04\n",
            "step   420 | loss: 1.171967 | lr: 5.04e-04\n",
            "step   430 | loss: 1.183441 | lr: 5.16e-04\n",
            "step   440 | loss: 1.193061 | lr: 5.28e-04\n",
            "step   450 | loss: 1.139350 | lr: 5.40e-04\n",
            "step   460 | loss: 1.146763 | lr: 5.52e-04\n",
            "step   470 | loss: 1.151897 | lr: 5.64e-04\n",
            "step   480 | loss: 1.153659 | lr: 5.76e-04\n",
            "step   490 | loss: 1.125368 | lr: 5.88e-04\n",
            "step   500 | loss: 1.130178 | lr: 6.00e-04\n",
            "step   510 | loss: 1.115436 | lr: 6.00e-04\n",
            "step   520 | loss: 1.109211 | lr: 6.00e-04\n",
            "step   530 | loss: 1.103120 | lr: 6.00e-04\n",
            "step   540 | loss: 1.096485 | lr: 6.00e-04\n",
            "step   550 | loss: 1.079472 | lr: 6.00e-04\n",
            "step   560 | loss: 1.072652 | lr: 6.00e-04\n",
            "step   570 | loss: 1.070345 | lr: 6.00e-04\n",
            "step   580 | loss: 1.063017 | lr: 6.00e-04\n",
            "step   590 | loss: 1.043877 | lr: 6.00e-04\n",
            "step   600 | loss: 1.030016 | lr: 6.00e-04\n",
            "step   610 | loss: 1.046767 | lr: 6.00e-04\n",
            "step   620 | loss: 1.027992 | lr: 6.00e-04\n",
            "step   630 | loss: 1.024515 | lr: 6.00e-04\n",
            "step   640 | loss: 0.997376 | lr: 6.00e-04\n",
            "step   650 | loss: 1.000514 | lr: 6.00e-04\n",
            "step   660 | loss: 0.992783 | lr: 6.00e-04\n",
            "step   670 | loss: 0.984819 | lr: 6.00e-04\n",
            "step   680 | loss: 0.966347 | lr: 5.99e-04\n",
            "step   690 | loss: 0.964715 | lr: 5.99e-04\n",
            "step   700 | loss: 0.950435 | lr: 5.99e-04\n",
            "step   710 | loss: 0.929466 | lr: 5.99e-04\n",
            "step   720 | loss: 0.937104 | lr: 5.99e-04\n",
            "step   730 | loss: 0.927359 | lr: 5.99e-04\n",
            "step   740 | loss: 0.911822 | lr: 5.99e-04\n",
            "step   750 | loss: 0.891194 | lr: 5.99e-04\n",
            "step   760 | loss: 0.914453 | lr: 5.99e-04\n",
            "step   770 | loss: 0.890748 | lr: 5.99e-04\n",
            "step   780 | loss: 0.861533 | lr: 5.99e-04\n",
            "step   790 | loss: 0.857927 | lr: 5.99e-04\n",
            "step   800 | loss: 0.880954 | lr: 5.99e-04\n",
            "step   810 | loss: 0.862137 | lr: 5.98e-04\n",
            "step   820 | loss: 0.815539 | lr: 5.98e-04\n",
            "step   830 | loss: 0.844857 | lr: 5.98e-04\n",
            "step   840 | loss: 0.846186 | lr: 5.98e-04\n",
            "step   850 | loss: 0.823721 | lr: 5.98e-04\n",
            "step   860 | loss: 0.769690 | lr: 5.98e-04\n",
            "step   870 | loss: 0.813815 | lr: 5.98e-04\n",
            "step   880 | loss: 0.808905 | lr: 5.98e-04\n",
            "step   890 | loss: 0.782712 | lr: 5.98e-04\n",
            "step   900 | loss: 0.735418 | lr: 5.97e-04\n",
            "step   910 | loss: 0.755567 | lr: 5.97e-04\n",
            "step   920 | loss: 0.769218 | lr: 5.97e-04\n",
            "step   930 | loss: 0.759394 | lr: 5.97e-04\n",
            "step   940 | loss: 0.711948 | lr: 5.97e-04\n",
            "step   950 | loss: 0.703180 | lr: 5.97e-04\n",
            "step   960 | loss: 0.733609 | lr: 5.97e-04\n",
            "step   970 | loss: 0.707067 | lr: 5.96e-04\n",
            "step   980 | loss: 0.672186 | lr: 5.96e-04\n",
            "step   990 | loss: 0.651803 | lr: 5.96e-04\n",
            "step  1000 | loss: 0.683628 | lr: 5.96e-04\n",
            "step  1010 | loss: 0.660144 | lr: 5.96e-04\n",
            "step  1020 | loss: 0.630731 | lr: 5.96e-04\n",
            "step  1030 | loss: 0.625338 | lr: 5.95e-04\n",
            "step  1040 | loss: 0.646030 | lr: 5.95e-04\n",
            "step  1050 | loss: 0.619552 | lr: 5.95e-04\n",
            "step  1060 | loss: 0.611383 | lr: 5.95e-04\n",
            "step  1070 | loss: 0.608145 | lr: 5.95e-04\n",
            "step  1080 | loss: 0.610523 | lr: 5.95e-04\n",
            "step  1090 | loss: 0.584547 | lr: 5.94e-04\n",
            "step  1100 | loss: 0.578926 | lr: 5.94e-04\n",
            "step  1110 | loss: 0.541326 | lr: 5.94e-04\n",
            "step  1120 | loss: 0.560376 | lr: 5.94e-04\n",
            "step  1130 | loss: 0.537621 | lr: 5.94e-04\n",
            "step  1140 | loss: 0.531101 | lr: 5.93e-04\n",
            "step  1150 | loss: 0.498466 | lr: 5.93e-04\n",
            "step  1160 | loss: 0.524599 | lr: 5.93e-04\n",
            "step  1170 | loss: 0.511528 | lr: 5.93e-04\n",
            "step  1180 | loss: 0.494453 | lr: 5.92e-04\n",
            "step  1190 | loss: 0.459361 | lr: 5.92e-04\n",
            "step  1200 | loss: 0.473305 | lr: 5.92e-04\n",
            "step  1210 | loss: 0.463251 | lr: 5.92e-04\n",
            "step  1220 | loss: 0.467562 | lr: 5.92e-04\n",
            "step  1230 | loss: 0.425730 | lr: 5.91e-04\n",
            "step  1240 | loss: 0.443016 | lr: 5.91e-04\n",
            "step  1250 | loss: 0.434190 | lr: 5.91e-04\n",
            "step  1260 | loss: 0.431964 | lr: 5.91e-04\n",
            "step  1270 | loss: 0.384551 | lr: 5.90e-04\n",
            "step  1280 | loss: 0.387341 | lr: 5.90e-04\n",
            "step  1290 | loss: 0.381133 | lr: 5.90e-04\n",
            "step  1300 | loss: 0.383674 | lr: 5.90e-04\n",
            "step  1310 | loss: 0.358160 | lr: 5.89e-04\n",
            "step  1320 | loss: 0.348932 | lr: 5.89e-04\n",
            "step  1330 | loss: 0.345927 | lr: 5.89e-04\n",
            "step  1340 | loss: 0.347343 | lr: 5.89e-04\n",
            "step  1350 | loss: 0.310912 | lr: 5.88e-04\n",
            "step  1360 | loss: 0.314042 | lr: 5.88e-04\n",
            "step  1370 | loss: 0.305233 | lr: 5.88e-04\n",
            "step  1380 | loss: 0.298112 | lr: 5.87e-04\n",
            "step  1390 | loss: 0.279920 | lr: 5.87e-04\n",
            "step  1400 | loss: 0.266745 | lr: 5.87e-04\n",
            "step  1410 | loss: 0.269075 | lr: 5.87e-04\n",
            "step  1420 | loss: 0.258365 | lr: 5.86e-04\n",
            "step  1430 | loss: 0.241280 | lr: 5.86e-04\n",
            "step  1440 | loss: 0.235531 | lr: 5.86e-04\n",
            "step  1450 | loss: 0.240970 | lr: 5.85e-04\n",
            "step  1460 | loss: 0.232372 | lr: 5.85e-04\n",
            "step  1470 | loss: 0.224779 | lr: 5.85e-04\n",
            "step  1480 | loss: 0.211237 | lr: 5.84e-04\n",
            "step  1490 | loss: 0.206955 | lr: 5.84e-04\n",
            "step  1500 | loss: 0.198114 | lr: 5.84e-04\n",
            "step  1510 | loss: 0.189291 | lr: 5.83e-04\n",
            "step  1520 | loss: 0.177042 | lr: 5.83e-04\n",
            "step  1530 | loss: 0.182023 | lr: 5.83e-04\n",
            "step  1540 | loss: 0.180288 | lr: 5.82e-04\n",
            "step  1550 | loss: 0.156798 | lr: 5.82e-04\n",
            "step  1560 | loss: 0.145820 | lr: 5.82e-04\n",
            "step  1570 | loss: 0.143844 | lr: 5.81e-04\n",
            "step  1580 | loss: 0.140487 | lr: 5.81e-04\n",
            "step  1590 | loss: 0.135439 | lr: 5.81e-04\n",
            "step  1600 | loss: 0.129402 | lr: 5.80e-04\n",
            "step  1610 | loss: 0.125610 | lr: 5.80e-04\n",
            "step  1620 | loss: 0.112688 | lr: 5.80e-04\n",
            "step  1630 | loss: 0.118619 | lr: 5.79e-04\n",
            "step  1640 | loss: 0.106881 | lr: 5.79e-04\n",
            "step  1650 | loss: 0.102593 | lr: 5.79e-04\n",
            "step  1660 | loss: 0.102219 | lr: 5.78e-04\n",
            "step  1670 | loss: 0.099636 | lr: 5.78e-04\n",
            "step  1680 | loss: 0.091915 | lr: 5.77e-04\n",
            "\n",
            "✓ Target loss < 0.99 achieved at step 1680!\n",
            "\n",
            "Final loss: 0.091915\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "0",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "train_loader = DataLoaderLite(B=32, T=256)\n",
        "\n",
        "# OPTIMIZED TRAINING CODE\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=6e-4, weight_decay=0.1)\n",
        "\n",
        "# Training hyperparameters\n",
        "max_steps = 10000\n",
        "eval_interval = 100\n",
        "log_interval = 10\n",
        "learning_rate = 6e-4\n",
        "warmup_steps = 500\n",
        "gradient_accumulation_steps = 4\n",
        "\n",
        "# Learning rate schedule with warmup and decay\n",
        "def get_lr(step):\n",
        "    if step < warmup_steps:\n",
        "        return learning_rate * (step + 1) / warmup_steps\n",
        "    # Cosine decay after warmup\n",
        "    progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    return learning_rate * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "# Training loop with gradient accumulation\n",
        "total_loss = 0\n",
        "best_loss = float('inf')\n",
        "avg_loss = 100.0\n",
        "\n",
        "for step in range(max_steps):\n",
        "    # Learning rate schedule\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # Get batch\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = model(x, y)\n",
        "\n",
        "    # Scale loss for gradient accumulation\n",
        "    loss = loss / gradient_accumulation_steps\n",
        "    loss.backward()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # Gradient accumulation: update weights every N steps\n",
        "    if (step + 1) % gradient_accumulation_steps == 0:\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Logging\n",
        "    if (step + 1) % log_interval == 0:\n",
        "        avg_loss = total_loss / log_interval\n",
        "        print(f'step {step + 1:5d} | loss: {avg_loss:.6f} | lr: {lr:.2e}')\n",
        "        total_loss = 0\n",
        "\n",
        "    # Check if target reached\n",
        "    if avg_loss < 0.099:\n",
        "        print(f\"\\n✓ Target loss < 0.99 achieved at step {step + 1}!\")\n",
        "        best_loss = avg_loss\n",
        "        torch.save(model.state_dict(), \"model_last.pt\")\n",
        "        break\n",
        "\n",
        "    # Early stopping if loss becomes too large\n",
        "    if math.isnan(avg_loss):\n",
        "        print(\"Loss became NaN, stopping training\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nFinal loss: {avg_loss:.6f}\")\n",
        "import sys; sys.exit(0)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)[0] # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
